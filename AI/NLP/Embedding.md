# 임베딩을 만드는 세 가지 철학  

1. Bag-of-Words hypothesis : 어떤 단어가 많이 쓰였는가? (TF-IDF etc)
2. Language Model : 단어가 어떤 순서로 쓰였는가? (ELMo, GPT, etc)
3. Distributional hypothesis : 어떤 단어가 동시에 출현하여 같이 쓰였는가? (word2vec, glove etc)


## 1. Bag of words 
- 단어 사용 여부나 단어 빈도를 중시하고 단어의 순서 정보는 무시한다. 
- 대표 통계량은 TF-IDF(Term Frequency-Inverse Document Frequency)이며, 딥러닝 버전은 Deep Averaging Network(Iyyer et al., 2015)다.

수학에서 Bag이란 <strong>중복 원소를 허용한 집합</strong>을 뜻한다. Bag of words 임베딩에는 '저자가 생각한 주제가 문서에서의 단어 사용에 녹아 있다'는 가정이 깔려있다. 이는 간단한 아이디어이지만 <strong>정보 검색 분야에서 많이 사용되고 있다.</strong> 

사용자의 질의에 가장 적절한 문서를 보여줄 때 질의를 Bag of words 임베딩으로 변환하고 <strong>질의와 검색 대상 문서 임베딩 간 코사인 유사도</strong>를 구해 유사도가 가장 높은 문서를 사용자에게 노출한다. 

- ### TF-IDF

<strong>단어 빈도 또는 등장 여부를 그대로 임베딩으로 사용하게 될 경우, '을/를', '이/가' 같은 조사가 많이 등장할 경우 주제를 파악하기 어려울 수 있다는 점이다. </strong> 이러한 단점을 보완하기 위해 제안된 기법이 TF-IDF이다. 
  
단어-문서 행렬에 아래 수식을 활용하여 가중치를 계산해 행렬 원소 (matrix element)를 바꾼다. 
$$
    TF-IDF(w) = TF(w) \times log(\frac{N}{DF(w)})
$$
  
- TF(Term Frequency): 어떤 단어가 특정 문서에 얼마나 많이 쓰였는지 빈도 
- DF(Document Frequency): 특정 단어가 나타난 문서의 수
- IDF(Inverse Document Frequency): 전체 문서 수(N)를 해당 단어 DF로 나눈 뒤 로그를 취한 값

- ### Deep Averaging Network

Bag of words 가정의 뉴럴 네트워크 버전으로, Bag of words 가정과 연결될 수 있는 지점은 단어의 순서를 고려하지 않는다는 점에 있다. <strong>간단한 구조임에도 성능이 좋아서 현업에서 주로 사용</strong>

## 2. Language model 
- 단어의 등장 순서를 학습해 주어진 단어 시퀀스가 얼마나 자연스러운지 확률을 부여한다. 
- Neural network 기반의 언어 모델이다.
- 대표 모델은 ELMo, GPT, BERT 등이 있다.

언어 모델(Language model)은 단어 시퀀스에 확률을 부여하는 모델이다. 단어가 $n$개 주어진 상황이라면 언어 모델은 $n$개 단어가 동시에 나타날 확률, 즉 $P(w_1,\,...\,,w_n)$ 을 반환한다. 

<strong>단점</strong> 
문법적으로나 의미적으로 결함이 없는 훌륭한 한국어 문장임에도 말뭉치에 한 번도 등장하지 않은 문장은 말이 되지 않는 문장으로 취급한다는 점

- ### 2.1 통계 기반 언어 모델

통계 기반 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습한다. 자연스러운 한국어 문장에 높은 확률 값을 부여한다. 

> 누명을 쓰다: 0.41
> 누명을 당하다: 0.02
> 
> 난폭 운전: 0.39
> 무모 운전: 0.01

#### 2.1.1. n-gram 
n-gram은 $n$개 단어를 뜻하는 용어이며 말뭉치 내 단어들을 $n$개씩 묶어서 그 빈도를 학습했다는 뜻이다. 이는 직전 $n-1$개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사하는 것이다. 

예시 - '내 마음 속에 영원히 기억될 최고의' 다음에 '명작이다'가 나타날 확률
$$
P(명작이다|내,\,마음,\,속에,\,영원히,\,,기억될,\,최고의,\,명작이다) = \frac{Freq(내,\,마음,\,속에,\,영원히,\,기억될,\,최고의,\,명작이다)}{Freq(내,\,마음,\,속에,\,영원히,\,기억될,\,최고의)}
$$

#### 2.1.1.1. bi-gram
bi-gram 은 $n=2$에 대한 n-gram 으로 직전의 1개 단어만 보고 전체 단어 시퀀스 등장 확률을 근사한 것이다. 

$$
P(w_n|w_{n-1}) = \frac{Freq(w_{n-1},w_n)}{Freq(w_{n-1})}\\
$$

$$
P(w_1^n) = P(w_1,w_2,\,...\,,w_n) = \prod_{k=1}^{n}P(w_k|w_{k-1})
$$

예시
$$
P(내,\,마음,\,영원히,\,기억될,\,최고의,\,명작이다) \approx P(내) \times P(마음|내) \times P(속에|마음) \times P(영원히|속에) \times P(기억될|영원히) \times P(최고의|기억될) \times P(명작이다|최고의)
$$

<strong>단점</strong>

데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 예측 단계에서 문제가 발생할 수 있다. <strong>즉, 한 번도 등장하지 않은 단어가 있다면, 언어 모델은 예측 단계에서 자연스런 한국어 문장이 등장할 확률을 0으로 부여하게 된다.</strong> 

#### 2.1.2. back-off (n-gram 방식 보완)

back-off란 n-gram 등장 빈도를 $n$보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식이다. 

$\alpha,\,\beta는 실제 빈도와의 차이를 보정해주는 파라미터이다. $

$$
Freq(내\,마음\,속에\,영원히\,기억될\,최고의\,명작이다) \approx \alpha\,Freq(영원히\,기억될\,최고의\,명작이다)+\beta
$$

#### 2.1.3. smoothing (n-gram 방식 보완)
 
등장 빈도 표에 모두 $k$ 만큼 더하는 기법이다. smoothing을 시행하면 <strong>높은 빈도를 가진 문자열 등장 확률을 일부 깎고 학습 데이터에 전혀 등장하지 않는 케이스들에는 작으나마 일부 확률을 부여하게 된다.</strong> 

빈도 수 0의 경우, $k = (0 + k)$ 가 되고, 만약 $k$ 를 1로 설정한다면 이를 특별히 <strong>laplace smoothing(라플라스 스무딩)</strong>이라고 한다.

- ### 2.2 Neural network 기반 언어 모델 (ELMo, GPT, BERT model)

뉴럴 네트워크는 입력과 출력 사이의 관계를 유연하게 포착해낼 수 있고, 그 자체로 확률 모델로서의 기능을 할 수 있다. 

<strong>뉴럴 네트워크 기반 언어 모델은 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습된다.</strong>

학습이 완료되면 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문장의 임베딩으로 활용한다. 

#### 2.2.1. Masked language model 

언어 모델 기법과 큰 틀에서 유사하지만 디테일에서 차이를 보이는 기법이다. 문장 중간에 마스크를 씌워 놓고, 해당 마스크 위치에 어떤 단어가 올지 예측하는 과정에서 학습한다. 

기존 언어 모델 기법들 대비 임베딩 품질이 좋다. BERT가 이 부류에 속한다. 

- 언어 모델 기반 기법 vs 마스크 언어 모델 기반 기법
    언어 모델 기반 기법은 <strong>순차적으로 입력받아 다음 단어를 맞춰야하므로 태생적으로 일방향이다.</strong> 반면에, 마스크 언어 모델 기반 기법은 <strong>문장 전체를 다 보고 중간에 있는 단어를 예측하기 때문에 양방향 학습이 가능하다.</strong>


## 3. Distributional hypothesis
- 문장에서 어떤 단어가 같이 쓰였는지를 중요하게 따진다. 
- 단어의 의미는 그 주변 문맥을 통해 유추해볼 수 있다고 보는 것이다. 
- 대표 통계량은 PMI (Pointwise Mutual Information, 점별 상호 정보량)이며, 대표 모델은 Word2Vec이 있다.

자연어 처리에서 분포란 특정 범위, 즉 윈도우 내에 동시에 등장하는 이웃 단어 또는 문맥의 집합을 가리킨다. <strong>Distributional hypothesis은 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는 전제를 가진다. </strong> 

> 특기 는 자칭 청소 와 빨래 지만 요리는 절망 적
> 재 를 우려낸 물 로 빨래 할 때 나 
>
> 찬 물 로 옷 을 세탁 한다.
> 세탁, 청소, 요리 와 가사 는 ...

Target word (타깃 단어): 관찰된 단어 (빨래 | 세탁)
Context word (문맥 단어): 그 주위에 등장한 단어 (청소, 요리, 물 | 청소, 요리, 물, 옷)

형태소: 의미를 가지는 최소 단위
품사: 단어를 문법적 성질의 공통성에 따라 몇 갈래로 묶어놓은 것 (기능, 의미, 형식) 

### 3.1.PMI (Pointwise Mutual Information, 점별 상호 정보량) 

<strong>두 확률변수 사이의 상관성을 계량화하는 단위다.</strong> 두 확률변수가 완전히 독립인 경우 그 값이 0이 된다. 단어 A가 등장할 때 단어 B와 자주 같이 나타난다면 PMI 값은 커진다.

$$
PMI(A,\,B) = log\frac{P(A,\,B)}{P(A) \times P(B)}
$$

<strong>PMI는 분포 가정에 따른 단어 가중치 할당 기법이다. 두 단어가 얼마나 자주 같이 등장하는지에 관한 정보를 수치화한 것이기 때문이다. </strong>이렇게 구축한 PMI 행렬의 행 벡터 자체를 해당 단어의 임베딩으로 사용할 수 있다.

예시 - window = 2인 단어 문맥 행렬

청소, 와, <strong>빨래</strong>, 지만, 요리는

<table>
    <thead>
        <tr>
            <td>단어/문맥</td>
            <td>청소</td>
            <td>와</td>
            <td>빨래</td>
            <td>지만</td>
            <td>요리는</td>
            <td>Total</td>
        </tr>
    </thead>
    <tbody> 
        <tr>
            <td>청소</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>...</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>빨래</td>
            <td>+1</td>
            <td>+1</td>
            <td></td>
            <td>+1</td>
            <td>+1</td>
            <td>20</td>
        </tr>
        <tr>
            <td>Total</td>
            <td>15</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td>1000</td>
        </tr>
    </tbody>
</table>

윈도우가 2라면 타깃 단어 앞뒤로 2개의 문맥 단어의 빈도를 계산한다. 
예를 들어, 빨래가 타깃 단어라면 '청소', '와', '지만', '요리는' 이라는 문맥 단어가 이번 빈도 계산의 대상이 되고 이들의 값을 1씩 올려준다. 

예시 - 빨래, 청소의 PMI 계산
$$
PMI(빨래, 청소) = log\frac{P(빨래, 청소)}{P(빨래) \times P(청소)} = log \frac{\frac{10}{1000}}{\frac{20}{1000} \times \frac{15}{1000}}
$$

### 3.2 Word2Vec
분포 가정의 대표적인 모델은 2013년 구글 연구 팀이 발표한 Word2Vec 임베딩 기법이다. CBOW 모델은 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습된다. Skip-gram 모델은 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습된다. 둘 모두 특정 타깃 단어 주변의 문맥, 즉 분포 정보를 임베딩에 함축한다.
